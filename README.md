# [*Video Explanation of our Project*](https://cmu.box.com/s/8mgpujfg59x5ory8w8dg12z1wqv5sl3r)

# Systems and Toolchains Course Project

### **Overview**
This project, completed as part of the **Systems and Toolchains** course at Carnegie Mellon University, involves working with the FIFA dataset to perform database management, analytics, machine learning, and cloud deployment tasks. **Grade: 107/100**, with additional credit for exceptional performance.

---

### **Key Skills**
- **Programming Languages:** Python, SQL  
- **Frameworks and Libraries:** PyTorch, Spark  
- **Database Management:** PostgreSQL (schema design, data ingestion, querying)  
- **Big Data Technologies:** Apache Spark (data pipelines, distributed processing)  
- **Cloud Deployment:** Analytics and ML tasks on cloud infrastructure  
---

### Dataset Description
The FIFA dataset, available on [Kaggle](https://www.kaggle.com/stefanoleone992/fifa-22-complete-player-dataset), contains soccer player statistics for 2015–2022:
- **Male player data:** Available for years 2015–2022.
- **Female player data:** Available for years 2016–2022.
- **Key Features:**
  - Player attributes such as age, nationality, club, contract details, and skill ratings.
  - Incomplete data for career mode and female players (excluded in analysis).

---

### Tasks Completed

#### **Task I: Database Management**
- **Objective:** Build and populate a PostgreSQL database table with the dataset.
- Steps:
  - Ingested data from multiple years into a unified PostgreSQL table under the `fifa` schema.
  - Ensured column alignment and added a `year` column to uniquely identify records.
  - Database allowed for seamless querying and integration with analytics pipelines.
  
- **Why PostgreSQL?**
  - **Structured Data:** The relational nature of PostgreSQL is ideal for structured datasets like FIFA.
  - **Consistency:** Ensures data integrity across multiple years.
  - **Ease of Querying:** SQL's expressive power is crucial for complex queries, which would be cumbersome in NoSQL databases.

---

#### **Task II: Big Data Analytics**
Developed Python functions utilizing Spark to answer the following questions:
1. **Player Contracts Analysis:**
   - Identified clubs with the highest number of players whose contracts ended after a user-specified year.
2. **Average Player Age Analysis:**
   - Found clubs with the highest or lowest average player age for a given year, handling ties at rank boundaries.
3. **Most Popular Nationality:**
   - Determined the most frequent nationality for players each year from 2015–2022.

---

#### **Task III: Machine Learning Modeling**
- **Objective:** Predict players' overall value based on their skillsets.
- Models Built:
  - **Spark MLlib:** Implemented two regressors with hyperparameter tuning.
  - **PyTorch:** Built two neural networks (e.g., shallow vs. deep) to compare performance.
- **Feature Engineering:** Included thorough data cleaning and preparation to improve model accuracy.
- **Key Results:** Documented the impact of tunable parameters and provided a comparative analysis of model performance.

---

#### **Task IV: Cloud Deployment**
- Deployed analytics and machine learning code to the cloud, skipping database creation for simplicity.
- Additional credit earned by deploying PostgreSQL on the cloud.

---

### How to Run the Code
1. **Clone the Repository:**
   ```bash
   git clone https://github.com/YourUsername/repository-name.git
   cd repository-name

| Micah Baldonado | Rolian Tan
|:----------------|------------------------:|



# Instructions:
1. We assemble all the functions, running process and output process in the 'final_sys_tools_project_code.ipynb' file, you can run it directly on the windows system.
2. the 'final_sys_tools_project_code-cloud.ipynb' is our results while runing on the cloud. 
3. Here are some path you need to modify:
    1. jdbc_driver_path = " " --- your jdbc .jar file
    2. dataset_folder_path  = " " --- your .csv data folder files
    3. db_properties['username']=" " --- postgres username
    4. db_properties['password']=" " --- passpord
    5. db_properties['url']= " " --- the url to link to the postgres
    6. db_properties['table']= " " --- the table name you want to write to
4. Make sure you have installed every libraries we imported. you can use 'pip install LIBRARY NAME' to install missing ones.
5. Finally, run the entire jupyter notebook code block by block, you could reimplement our results
6. If you have trouble doing this, you can refer to our demo video or create issues in this github repository. 

# Task 1
## Feature Description:

- `id`: Identifier combined by `sofifa_id` and `year`, unique (text)
- `sofifa_id`: Fifa player identifier (numeric)
- `player_url`: URL to the player's profile (text)
- `short_name`: Short name of the player (text)
- `long_name`: Full name of the player (text)
- `player_positions`: Positions the player can play (multiple positions, separated by commas) (text)
- `overall`: Overall player rating, 0-100 (numeric)
- `potential`: Potential player rating, 0-100 (numeric)
- `value_eur`: Market value of the player, Euros (numeric)
- `wage_eur`: Weekly wage of the player, Euros (numeric)
- `age`: Age of the player (numeric)
- `dob`: Date of birth of the player (date, yyyy-mm-dd)
- `height_cm`: Height of the player, centimeters (numeric)
- `weight_kg`: Weight of the player, kilograms (numeric)
- `club_team_id`: Club identifier, unique (numeric)
- `club_name`: Name of the club the player belongs to (text)
- `league_name`: Name of the league the club belongs to (text)
- `league_level`: Level of the league (numeric)
- `club_position`: Position the player plays in the club (text)
- `club_jersey_number`: Jersey number of the player (numeric)
- `club_loaned_from`: Name of the club the player is loaned from (text)
- `club_joined`: Date the player joined the club (date, yyyy-mm-dd)
- `club_contract_valid_until`: Year the player's contract is end (numeric)
- `nationality_id`: Nationality identifier, unique (numeric)
- `nationality_name`: Name of the player's nationality (text)
- `nation_team_id`: National team identifier, unique (numeric)
- `nation_position`: Position the player plays in the national team (text)
- `nation_jersey_number`: Jersey number of the player in the national team (numeric)
- `preferred_foot`: Player's preferred foot (text)
- `weak_foot`: Rating for the player's weaker foot ability, 1-5 (numeric)
- `skill_moves`: Rating for the player's skill moves, 1-5 (numeric)
- `international_reputation`: Rating for the player's international reputation, 1-5 (numeric)
- `work_rate`: Player's work rate, format (attack/defense) (text)
- `body_type`: Player's body type description (text)
- `real_face`: Whether the game has the player's real face data, 'Yes' or 'No' (text)
- `release_clause_eur`: Release clause value of the player, Euros (numeric)
- `player_tags`: Player's special skills (text)
- `player_traits`: The player's traits on the field (text)
- `pace`, `shooting`, `passing`, `dribbling`, `defending`: Rating for the player's [pace, shooting, passing, dribbling, defending], 0-100 (numeric)
- `physic`: Rating for the player's physical strength, 0-100 (numeric)
  
- `attacking_crossing`, `attacking_finishing`, `attacking_heading_accuracy`, `attacking_short_passing`,`attacking_volleys` : Rating for the player's attacking ability, from the aspects of [crossing, finishing, heading_accuracy, short_passing, volleys], 0-100 (numeric)

- `skill_dribbling`, `skill_curve`, `skill_fk_accuracy`, `skill_long_passing`, `skill_ball_control`: Rating for the player's ball skill from aspects of [dribbling, curve passing, free-kick, long passing, ball control], 0-100 (numeric)

- `movement_acceleration`, `movement_sprint_speed`, `movement_agility`, `movement_reactions`, `movement_balance`: Rating for the player's movement anilities from aspects of [acceleration, sprint speed, agility, reactions, balance], 0-100 (numeric)

- `power_shot_power`, `power_jumping`, `power_stamina`, `power_strength`, `power_long_shots`: Rating for the player's power from aspects of [shot, jumping, stamina, physical strength, long shots], 0-100 (numeric)

- `mentality_aggression`, `mentality_interceptions`, `mentality_positioning`, `mentality_vision`, `mentality_penalties`, `mentality_composure`: Rating for the player's mentality realated to aspects of [aggression, interceptions, positioning, vision, penalty-taking, composure]

- `defending_marking_awareness`: Rating for the player's marking awareness, 0-100 (numeric)
- `defending_standing_tackle`: Rating for the player's standing tackle ability, 0-100 (numeric)
- `defending_sliding_tackle`: Rating for the player's sliding tackle ability, 0-100 (numeric)


## Reasons to use PostgreSQL rather than NoSQL database:
1. Complex Queries and Aggregation
PostgreSQL supports efficient joins and aggregations, allowing to easily query across tables (e.g finding clubs by years, calculating averages). NoSQL databases often require more complex operations for similar queries.
2. Data Integrity Validation
With ACID compliance and support for constraints, PostgreSQL ensures accurate and consistent data (e.g valid age ranges or player ratings), which is harder to enforce in most NoSQL systems.
3. Feature-Related Dataset
Since this dataset contains player bio information, player ability information and club information, so PostgreSQL has more potential to create multiple sub-tables by using unique foreign keys. However, the NoSQL is a non-relation method.

# Task 3 - Machine Leaning
# Attention: Due to the different experiemnt results on local machine and on the cloud, we will use the local outputs as refernece. 
## Since this is a regression task so we choose regressors
### Pyspark model:
1. we use two mainstream regressor: DecisionTree Regressor, RandomForest Regressor
2. `DecisionTree`: Simplicity and ease of interpretability, making it suitable for quick insights.
3. `RandomForest`: Robustness and ability to reduce overfitting through ensemble learning, providing more reliable insights.
### Pyspark Parameter Tuning:
We use Paramgrid to find the best pairs of parameters as the tuning method. 
1. `DecisionTree`: mean absolute error (MAE) of 0.1869
2. `RandomForest`: a slightly higher MAE of 0.2003, may because of the limitation of the hyperparmeter settings or the performance threshold of the tree structure regressors on this large dataset.  
Here are the explaination of the hyper parametes:
1. `maxDepth`: Controls the depth of the trees. A higher maxDepth enables the model to capture more complex patterns, but it can lead to overfitting if set too high. For both models, tuning maxDepth was essential to balance model complexity and generalization.
2. `numTrees`: Specific to the Random Forest model, numTrees defines how many decision trees are used. More trees generally lead to better performance, but with diminishing returns and increased computational cost.
### Pytorch model:
1. We create two neural networks to compare: shallow NN, deep NN  
2. `SimpleNN`: Testing the model’s performance with fewer parameters and a simpler architecture. Leading to faster training times and less risk of overfitting on smaller datasets.  
3. `DeepNN`: More complex architecture with three hidden layers, which is designed to capture more intricate patterns or high-level features in the data.  
### Pytorch Parameter Tuning:  
  1. `Batch Size (bs)`: this minibatch could excellerate the training process and won't exceed the computation resource limits  
  2. `Learning Rate (lr)`: determines the pace of gradients updating, when lr goes from 1e-2 to 1e-3, with epochs fixed, it brings a 28% MSE drop for simpleNN (from 0.251 to 0.178) and a 33% MSE drop for deepNN (from 0.153 to 0.107)  
  3. `Epochs`: higher epochs will provide more times of training to make sure the model converges, when lr=1e-5, incresing the epochs from 20 to 40 brings no MSE drop for both simpleNN (0.251->0.297) and deepNN(0.153->0.161), which means the models are already converged.   
  4. `model_type`: this decides which model to use  
  compare with `simpleNN` and `deepNN`, we will find that: when the hyperparameter are the same, the deepNN is more complicated to learn more   features compared with simpleNN, so the deepNN always reach better MSE scores. 
    
